# -*- coding: utf-8 -*-
"""Pre_traning Final-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16LdqXwgpXeUZYXAqNnsAlg0PRoHfXGS3
"""

from transformers import BertTokenizer, BertForPreTraining
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForPreTraining.from_pretrained('bert-base-uncased')
#model = BertForPreTraining.from_pretrained('medicalai/ClinicalBERT')

import pandas as pd
df = pd.read_csv(r"D:\Downloads\concatenate_colum.csv")
fd = pd.read_csv(r"D:\Downloads\Datasets\mayo_clinic.csv")

ff = pd.read_csv(r'D:\Downloads\merge_kaggle.tsv', sep='\t')

ff['concatenated'] = " Diseases "+ff.iloc[:, 1].astype(str)+" Symptoms " + ff.iloc[:, 2].astype(str) +" Drug "+ ff.iloc[:, 3].astype(str)

length_review = []
text= []
for i in df.review:
  length_review.append(len(i))
  text.append(i)

for i in fd.Review:
  length_review.append(len(i))
  text.append(i)

for i in ff.concatenated:
  length_review.append(len(i))
  text.append(i)

# hh = pd.DataFrame(text)
def write_list_to_file(lines, file_path):
    with open(file_path, 'w') as file:
        for line in lines:
            file.write(line + '\n')

# Example list of strings

# Path to the output text file
# output_file_path = "pre_train.txt"

# # Write the list to the text file
# write_list_to_file(text, output_file_path)


# print("Text file generated successfully.")

# len(hh)

# hh.iloc[915250,0]

import re

bag = [item for sentence in text for item in sentence.replace('\n', '.').split('.') if item != '']
bag_size = len(bag)

# bag = [item for sentence in text for item in sentence.replace('\n', '.').split('.') if item != '' and len(item.strip()) > 15]
# bag_size = len(bag)

import random

sentence_a = []
sentence_b = []
label = []

for paragraph in text:
    sentences = [
        sentence for sentence in paragraph.split('.') if sentence != ''
    ]
    num_sentences = len(sentences)
    if num_sentences > 1:
        start = random.randint(0, num_sentences-2)
        # 50/50 whether is IsNextSentence or NotNextSentence
        if random.random() >= 0.5:
            # this is IsNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(sentences[start+1])
            label.append(0)
        else:
            index = random.randint(0, bag_size-1)
            # this is NotNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(bag[index])
            label.append(1)

inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt',
                   max_length=512, truncation=True, padding='max_length')

inputs['next_sentence_label'] = torch.LongTensor([label]).T

inputs['labels'] = inputs.input_ids.detach().clone()

rand = torch.rand(inputs.input_ids.shape)
# create mask array
mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \
           (inputs.input_ids != 102) * (inputs.input_ids != 0)

selection = []

for i in range(inputs.input_ids.shape[0]):
    selection.append(
        torch.flatten(mask_arr[i].nonzero()).tolist()
    )

for i in range(inputs.input_ids.shape[0]):
    inputs.input_ids[i, selection[i]] = 103

class OurDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)

dataset = OurDataset(inputs)

loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)



from tqdm import tqdm
import torch
import torch.optim as optim
import pickle
# Define the device to run the model on
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
epochs = 5

# Create the optimizer
optimizer = optim.AdamW(model.parameters(), lr=0.001)

model = model.to(device)
# List to store loss values
losses = []

for epoch in range(epochs):
    # setup loop with TQDM and dataloader
    loop = tqdm(loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optimizer.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        next_sentence_label = batch['next_sentence_label'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids,
                        next_sentence_label=next_sentence_label,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optimizer.step()
        # Append loss to the list
        losses.append(loss.item())
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())


    # Save the model after each epoch
    torch.save(model.state_dict(), f"model_epoch_{epoch+1}.pt")

print("Training finished!")

# Now 'losses' list contains all loss values during training
with open("losses.pkl", "wb") as f:
    pickle.dump(losses, f)

print("Losses pickled!")

import torch

# Assuming 'model' is your PyTorch model
# Define the file path where you want to save the model
model_path = '/content/drive/MyDrive/my_model.pt'

# Save the model
model.save_pretrained(model_path)

print("BERT model saved successfully!")

torch.save(model.state_dict(), model_path)

print("Model saved successfully!")
