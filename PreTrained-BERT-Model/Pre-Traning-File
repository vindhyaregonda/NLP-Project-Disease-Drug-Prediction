from transformers import BertTokenizer, BertForPreTraining
import torch
import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForPreTraining.from_pretrained('bert-base-uncased')
#model = BertForPreTraining.from_pretrained('medicalai/ClinicalBERT')> Manav.log 2>&1

import pandas as pd
df = pd.read_csv("/home/seraj/project_dataset/concatenate_colum.csv")
fd = pd.read_csv("/home/seraj/surabhi_protein_seq/project/sur/mayo_clinic.csv")

ff = pd.read_csv('/home/seraj/surabhi_protein_seq/project/sur/merge_kaggle.tsv', sep='\t')
ff['concatenated'] = " Diseases "+ff.iloc[:, 1].astype(str)+" Symptoms " + ff.iloc[:, 2].astype(str) +" Drug "+ ff.iloc[:, 3].astype(str)

length_review = []
text= []
for i in df.review:
  length_review.append(len(i))
  text.append(i)

for i in fd.Review:
  length_review.append(len(i))
  text.append(i)

for i in ff.concatenated:
  length_review.append(len(i))
  text.append(i)

# hh = pd.DataFrame(text)

# len(hh)

# hh.iloc[915250,0]

import re

bag = [item for sentence in text for item in sentence.replace('\n', '.').split('.') if item != '']
bag_size = len(bag)

# bag = [item for sentence in text for item in sentence.replace('\n', '.').split('.') if item != '' and len(item.strip()) > 15]
# bag_size = len(bag)

import random

sentence_a = []
sentence_b = []
label = []

for paragraph in text:
    sentences = [
        sentence for sentence in paragraph.split('.') if sentence != ''
    ]
    num_sentences = len(sentences)
    if num_sentences > 1:
        start = random.randint(0, num_sentences-2)
        # 50/50 whether is IsNextSentence or NotNextSentence
        if random.random() >= 0.5:
            # this is IsNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(sentences[start+1])
            label.append(0)
        else:
            index = random.randint(0, bag_size-1)
            # this is NotNextSentence
            sentence_a.append(sentences[start])
            sentence_b.append(bag[index])
            label.append(1)

inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt',
                   max_length=512, truncation=True, padding='max_length')

inputs['next_sentence_label'] = torch.LongTensor([label]).T

inputs['labels'] = inputs.input_ids.detach().clone()

rand = torch.rand(inputs.input_ids.shape)
# create mask array
mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \
           (inputs.input_ids != 102) * (inputs.input_ids != 0)

selection = []

for i in range(inputs.input_ids.shape[0]):
    selection.append(
        torch.flatten(mask_arr[i].nonzero()).tolist()
    )

for i in range(inputs.input_ids.shape[0]):
    inputs.input_ids[i, selection[i]] = 103

class OurDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)

dataset = OurDataset(inputs)

loader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)



from tqdm import tqdm
import torch
import torch.optim as optim
import pickle
# Define the device to run the model on
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
epochs = 5

# Create the optimizer
optimizer = optim.AdamW(model.parameters(), lr=0.001)

model = model.to(device)
# List to store loss values
losses = []

for epoch in range(epochs):
    # setup loop with TQDM and dataloade

    loop = tqdm(loader, leave=True)
    for batch in loop:
        # initialize calculated gradients (from prev step)
        optimizer.zero_grad()
        # pull all tensor batches required for training
        input_ids = batch['input_ids'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        next_sentence_label = batch['next_sentence_label'].to(device)
        labels = batch['labels'].to(device)
        # process
        outputs = model(input_ids, attention_mask=attention_mask,
                        token_type_ids=token_type_ids,
                        next_sentence_label=next_sentence_label,
                        labels=labels)
        # extract loss
        loss = outputs.loss
        # calculate loss for every parameter that needs grad update
        loss.backward()
        # update parameters
        optimizer.step()
        # Append loss to the list
        losses.append(loss.item())
        # print relevant info to progress bar
        loop.set_description(f'Epoch {epoch}')
        loop.set_postfix(loss=loss.item())


    # Save the model after each epoch
    torch.save(model.state_dict(), f"model_epoch__mod{epoch+1}.pt")

print("Training finished!")

# Now 'losses' list contains all loss values during training
with open("losses_mod.pkl", "wb") as f:
    pickle.dump(losses, f)

print("Losses pickled!")
model.save_pretrained('./my_model')
